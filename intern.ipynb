{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Vishnu\\Downloads/Input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    html_text = requests.get(url, headers={'User-Agent': 'My User Agent 1.0'})\n",
    "    soup = BeautifulSoup(html_text.text, 'lxml')\n",
    "\n",
    "    if(html_text.status_code==200): # if the webpage exists or not\n",
    "        article_title = soup.title.text \n",
    "        article_content = soup.find('div', class_= 'td-post-content').text\n",
    "        \n",
    "        content.append(article_content) # storing article contents into a list\n",
    "\n",
    "        # writing the content and title into text files\n",
    "        with open(f\"files/{url_id}.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(f'Article title: \\n{article_title} \\n')\n",
    "            f.write(f'Article content:\\n {article_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "57\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# URL IDs in which the web pages are unavailable\n",
    "for i,row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    html_text = requests.get(url, headers={'User-Agent': 'My User Agent 1.0'})\n",
    "    soup = BeautifulSoup(html_text.text, 'lxml')\n",
    "\n",
    "    if(html_text.status_code!=200):\n",
    "        print(url_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the rows with invalid webpage links\n",
    "df = df[(df.URL_ID!=44) & (df.URL_ID!=57) & (df.URL_ID!=144)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vishnu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vishnu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI  | Afghanistan',\n",
       " 'ARIARY | Madagascar',\n",
       " 'BAHT | Thailand',\n",
       " 'BALBOA | Panama',\n",
       " 'BIRR | Ethiopia',\n",
       " 'BOLIVAR | Venezuela',\n",
       " 'BOLIVIANO  | Bolivia',\n",
       " 'CEDI | Ghana',\n",
       " 'COLON  | Costa Rica',\n",
       " 'C�RDOBA  | Nicaragua',\n",
       " 'DALASI | Gambia',\n",
       " 'DENAR | Macedonia (Former Yug. Rep.)',\n",
       " 'DINAR | Algeria',\n",
       " 'DIRHAM  | Morocco',\n",
       " 'DOBRA | S�o Tom and Pr�ncipe',\n",
       " 'DONG | Vietnam',\n",
       " 'DRAM | Armenia',\n",
       " 'ESCUDO  | Cape Verde',\n",
       " 'EURO  | Belgium',\n",
       " 'FLORIN | Aruba',\n",
       " 'FORINT | Hungary',\n",
       " 'GOURDE | Haiti',\n",
       " 'GUARANI | Paraguay',\n",
       " 'GULDEN | Netherlands Antilles',\n",
       " 'HRYVNIA  | Ukraine',\n",
       " 'KINA | Papua New Guinea',\n",
       " 'KIP | Laos',\n",
       " 'KONVERTIBILNA MARKA  | Bosnia-Herzegovina',\n",
       " 'KORUNA  | Czech Republic',\n",
       " 'KRONA | Sweden',\n",
       " 'KRONE | Denmark',\n",
       " 'KROON | Estonia',\n",
       " 'KUNA | Croatia',\n",
       " 'KWACHA | Zambia',\n",
       " 'KWANZA | Angola',\n",
       " 'KYAT | Myanmar',\n",
       " 'LARI | Georgia',\n",
       " 'LATS | Latvia',\n",
       " 'LEK | Albania',\n",
       " 'LEMPIRA | Honduras',\n",
       " 'LEONE | Sierra Leone',\n",
       " 'LEU | Romania',\n",
       " 'LEV | Bulgaria',\n",
       " 'LILANGENI  | Swaziland',\n",
       " 'LIRA | Lebanon',\n",
       " 'LITAS | Lithuania',\n",
       " 'LOTI | Lesotho',\n",
       " 'MANAT | Azerbaijan',\n",
       " 'METICAL | Mozambique',\n",
       " 'NAIRA | Nigeria',\n",
       " 'NAKFA | Eritrea',\n",
       " 'NEW LIRA | Turkey',\n",
       " 'NEW SHEQEL  | Israel',\n",
       " 'NGULTRUM  | Bhutan',\n",
       " 'NUEVO SOL | Peru',\n",
       " 'OUGUIYA  | Mauritania',\n",
       " 'PATACA | Macau',\n",
       " 'PESO  | Mexico',\n",
       " 'POUND  | Egypt',\n",
       " 'PULA  | Botswana',\n",
       " 'QUETZAL | Guatemala',\n",
       " 'RAND | South Africa',\n",
       " 'REAL  | Brazil',\n",
       " 'RENMINBI  | China',\n",
       " 'RIAL | Iran',\n",
       " 'RIEL | Cambodia',\n",
       " 'RINGGIT | Malaysia',\n",
       " 'RIYAL | Saudi Arabia',\n",
       " 'RUBLE | Russia',\n",
       " 'RUFIYAA | Maldives',\n",
       " 'RUPEE  | India',\n",
       " 'RUPEE  | Pakistan',\n",
       " 'RUPIAH  | Indonesia',\n",
       " 'SHILLING  | Uganda',\n",
       " 'SOM | Uzbekistan',\n",
       " 'SOMONI  | Tajikistan',\n",
       " 'SPECIAL DRAWING RIGHTS  | International Monetary Fund',\n",
       " 'TAKA | Bangladesh',\n",
       " 'TALA | Western Samoa',\n",
       " 'TENGE | Kazakhstan',\n",
       " 'TUGRIK  | Mongolia',\n",
       " 'VATU | Vanuatu',\n",
       " 'WON  | Korea, South',\n",
       " 'YEN | Japan',\n",
       " 'ZLOTY | Poland',\n",
       " 'HUNDRED  | Denominations',\n",
       " 'THOUSAND',\n",
       " 'MILLION',\n",
       " 'BILLION',\n",
       " 'TRILLION',\n",
       " 'DATE  | Time related',\n",
       " 'ANNUAL',\n",
       " 'ANNUALLY',\n",
       " 'ANNUM',\n",
       " 'YEAR',\n",
       " 'YEARLY',\n",
       " 'QUARTER',\n",
       " 'QUARTERLY',\n",
       " 'QTR',\n",
       " 'MONTH',\n",
       " 'MONTHLY',\n",
       " 'WEEK',\n",
       " 'WEEKLY',\n",
       " 'DAY',\n",
       " 'DAILY',\n",
       " 'JANUARY  | Calendar',\n",
       " 'FEBRUARY',\n",
       " 'MARCH',\n",
       " 'APRIL',\n",
       " 'MAY',\n",
       " 'JUNE',\n",
       " 'JULY',\n",
       " 'AUGUST',\n",
       " 'SEPTEMBER',\n",
       " 'OCTOBER',\n",
       " 'NOVEMBER',\n",
       " 'DECEMBER',\n",
       " 'JAN',\n",
       " 'FEB',\n",
       " 'MAR',\n",
       " 'APR',\n",
       " 'MAY',\n",
       " 'JUN',\n",
       " 'JUL',\n",
       " 'AUG',\n",
       " 'SEP',\n",
       " 'SEPT',\n",
       " 'OCT',\n",
       " 'NOV',\n",
       " 'DEC',\n",
       " 'MONDAY',\n",
       " 'TUESDAY',\n",
       " 'WEDNESDAY',\n",
       " 'THURSDAY',\n",
       " 'FRIDAY',\n",
       " 'SATURDAY',\n",
       " 'SUNDAY',\n",
       " 'ONE  | Numbers',\n",
       " 'TWO',\n",
       " 'THREE',\n",
       " 'FOUR',\n",
       " 'FIVE',\n",
       " 'SIX',\n",
       " 'SEVEN',\n",
       " 'EIGHT',\n",
       " 'NINE',\n",
       " 'TEN',\n",
       " 'ELEVEN',\n",
       " 'TWELVE',\n",
       " 'THIRTEEN',\n",
       " 'FOURTEEN',\n",
       " 'FIFTEEN',\n",
       " 'SIXTEEN',\n",
       " 'SEVENTEEN',\n",
       " 'EIGHTEEN',\n",
       " 'NINETEEN',\n",
       " 'TWENTY',\n",
       " 'THIRTY',\n",
       " 'FORTY',\n",
       " 'FIFTY',\n",
       " 'SIXTY',\n",
       " 'SEVENTY',\n",
       " 'EIGHTY',\n",
       " 'NINETY',\n",
       " 'FIRST',\n",
       " 'SECOND',\n",
       " 'THIRD',\n",
       " 'FOURTH',\n",
       " 'FIFTH',\n",
       " 'SIXTH',\n",
       " 'SEVENTH',\n",
       " 'EIGHTH',\n",
       " 'NINTH',\n",
       " 'TENTH',\n",
       " 'I  | Roman numerals',\n",
       " 'II',\n",
       " 'III',\n",
       " 'IV',\n",
       " 'V',\n",
       " 'VI',\n",
       " 'VII',\n",
       " 'VIII',\n",
       " 'IX',\n",
       " 'X',\n",
       " 'XI',\n",
       " 'XII',\n",
       " 'XIII',\n",
       " 'XIV',\n",
       " 'XV',\n",
       " 'XVI',\n",
       " 'XVII',\n",
       " 'XVIII',\n",
       " 'XIX',\n",
       " 'XX',\n",
       " 'ABOUT',\n",
       " 'ABOVE',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AMONG',\n",
       " 'AN',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEEN',\n",
       " 'BEFORE',\n",
       " 'BEING',\n",
       " 'BELOW',\n",
       " 'BETWEEN',\n",
       " 'BOTH',\n",
       " 'BUT',\n",
       " 'BY',\n",
       " 'CAN',\n",
       " 'DID',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DOWN',\n",
       " 'DURING',\n",
       " 'EACH',\n",
       " 'FEW',\n",
       " 'FOR',\n",
       " 'FROM',\n",
       " 'FURTHER',\n",
       " 'HAD',\n",
       " 'HAS',\n",
       " 'HAVE',\n",
       " 'HAVING',\n",
       " 'HE',\n",
       " 'HER',\n",
       " 'HERE',\n",
       " 'HERS',\n",
       " 'HERSELF',\n",
       " 'HIM',\n",
       " 'HIMSELF',\n",
       " 'HIS',\n",
       " 'HOW',\n",
       " 'IF',\n",
       " 'IN',\n",
       " 'INTO',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'ITS',\n",
       " 'ITSELF',\n",
       " 'JUST',\n",
       " 'ME',\n",
       " 'MORE',\n",
       " 'MOST',\n",
       " 'MY',\n",
       " 'MYSELF',\n",
       " 'NO',\n",
       " 'NOR',\n",
       " 'NOT',\n",
       " 'NOW',\n",
       " 'OF',\n",
       " 'OFF',\n",
       " 'ON',\n",
       " 'ONCE',\n",
       " 'ONLY',\n",
       " 'OR',\n",
       " 'OTHER',\n",
       " 'OUR',\n",
       " 'OURS',\n",
       " 'OURSELVES',\n",
       " 'OUT',\n",
       " 'OVER',\n",
       " 'OWN',\n",
       " 'SAME',\n",
       " 'SHE',\n",
       " 'SHOULD',\n",
       " 'SO',\n",
       " 'SOME',\n",
       " 'SUCH',\n",
       " 'THAN',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'THEIR',\n",
       " 'THEIRS',\n",
       " 'THEM',\n",
       " 'THEMSELVES',\n",
       " 'THEN',\n",
       " 'THERE',\n",
       " 'THESE',\n",
       " 'THEY',\n",
       " 'THIS',\n",
       " 'THOSE',\n",
       " 'THROUGH',\n",
       " 'TO',\n",
       " 'TOO',\n",
       " 'UNDER',\n",
       " 'UNTIL',\n",
       " 'UP',\n",
       " 'VERY',\n",
       " 'WAS',\n",
       " 'WE',\n",
       " 'WERE',\n",
       " 'WHAT',\n",
       " 'WHEN',\n",
       " 'WHERE',\n",
       " 'WHICH',\n",
       " 'WHILE',\n",
       " 'WHO',\n",
       " 'WHOM',\n",
       " 'WHY',\n",
       " 'WITH',\n",
       " 'YOU',\n",
       " 'YOUR',\n",
       " 'YOURS',\n",
       " 'YOURSELF',\n",
       " 'YOURSELVES',\n",
       " 'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'would',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero',\n",
       " 'UNITED  | Geographic',\n",
       " 'STATE',\n",
       " 'NORTH',\n",
       " 'SOUTH',\n",
       " 'EAST',\n",
       " 'NORTHEAST',\n",
       " 'NORTHWEST',\n",
       " 'SOUTHEAST',\n",
       " 'SOUTHWEST',\n",
       " 'WEST',\n",
       " 'OCEAN',\n",
       " 'SEA',\n",
       " 'LAKE',\n",
       " 'RIVER',\n",
       " 'CREEK',\n",
       " 'GULF',\n",
       " 'MOUNTAIN',\n",
       " 'STREET',\n",
       " 'BOULEVARD',\n",
       " 'BLVD',\n",
       " 'PARKWAY',\n",
       " 'CITY',\n",
       " 'COUNTY',\n",
       " 'COUNTRY',\n",
       " 'PACIFIC',\n",
       " 'ATLANTIC',\n",
       " 'INDIAN',\n",
       " 'MEDITERRANEAN',\n",
       " 'COMMONWEALTH',\n",
       " 'AMERICA',\n",
       " 'AMERICAN',\n",
       " 'YORK  | Cities',\n",
       " 'CHICAGO',\n",
       " 'LAS',\n",
       " 'VEGAS',\n",
       " 'LOS',\n",
       " 'ANGELES',\n",
       " 'MILWAUKEE',\n",
       " 'SUNNYVALE',\n",
       " 'FREMONT',\n",
       " 'CINCINNATI',\n",
       " 'PHILADELPHIA',\n",
       " 'MIAMI',\n",
       " 'DALLAS',\n",
       " 'FORT',\n",
       " 'BOSTON',\n",
       " 'HOUSTON',\n",
       " 'WASHINGTON',\n",
       " 'ATLANTA',\n",
       " 'DETROIT',\n",
       " 'SAN',\n",
       " 'FRANSICO',\n",
       " 'PHOENIX',\n",
       " 'SEATTLE',\n",
       " 'DIEGO',\n",
       " 'MINNEAPOLIS',\n",
       " 'MEMPHIS',\n",
       " 'DENVER',\n",
       " 'ST',\n",
       " 'LOUIS',\n",
       " 'PITTSBURGH',\n",
       " 'MANHATTAN',\n",
       " 'HOLLYWOOD',\n",
       " 'COLUMBUS',\n",
       " 'INDIANAPOLIS',\n",
       " 'MUMBAI',\n",
       " 'KARACHI',\n",
       " 'ONTARIO',\n",
       " 'TORONTO',\n",
       " 'CAMBRIDGE',\n",
       " 'DELHI',\n",
       " 'SAO',\n",
       " 'PAULO',\n",
       " 'SHANGHAI',\n",
       " 'MOSCOW',\n",
       " 'SEOUL',\n",
       " 'ISTANBUL',\n",
       " 'TOKYO',\n",
       " 'JAKARTA',\n",
       " 'BEIJING',\n",
       " 'LONDON',\n",
       " 'LUXEMBOURG',\n",
       " 'SINGAPORE',\n",
       " 'REPUBLIC  | Countries',\n",
       " 'CHINA',\n",
       " 'INDIA',\n",
       " 'INDONESIA',\n",
       " 'BRAZIL',\n",
       " 'BRAZILIAN',\n",
       " 'PAKISTAN',\n",
       " 'BANGLADESH',\n",
       " 'RUSSIA',\n",
       " 'NIGERIA',\n",
       " 'NOVA',\n",
       " 'SCOTIA',\n",
       " 'JAPAN',\n",
       " 'MALAYSIA',\n",
       " 'MEXICO',\n",
       " 'MEXICAN',\n",
       " 'PHILIPPINES',\n",
       " 'VIETNAM',\n",
       " 'GERMANY',\n",
       " 'FRANCE',\n",
       " 'KOREA',\n",
       " 'SPAIN',\n",
       " 'ARGENTINA',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the contents of custom stopwords into stopwords array\n",
    "\n",
    "import codecs\n",
    "import ftfy\n",
    "\n",
    "folder_path = 'D:\\Me\\Python stuff\\StopWords'\n",
    "\n",
    "# Create an empty list to store the contents of the text files\n",
    "stopwords = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    # Only read text files (skip other files)\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Try reading the file using UTF-8 encoding\n",
    "        try:\n",
    "            with codecs.open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    stopwords.append(line.strip())\n",
    "        except UnicodeDecodeError:\n",
    "            # If the file is not UTF-8 encoded, try repairing it and reading it again\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = f.read()\n",
    "            fixed_data = ftfy.fix_text(data)\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(fixed_data)\n",
    "            with codecs.open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    stopwords.append(line.strip())\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "cleaned_content = [' ']*len(content)\n",
    "for i in range(len(content)):\n",
    "      for w in stopwords:\n",
    "        cleaned_content[i] = content[i].replace(' '+ w +' ',' ').replace('?','').replace('.','').replace(',','').replace('!','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing no of sentences into an array\n",
    "sentences = []\n",
    "for i in content:\n",
    "    sentences.append(len(sent_tokenize(i)))\n",
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing no of words of each article into an array\n",
    "words = []\n",
    "for i in content:\n",
    "  words.append(len(word_tokenize(i)))\n",
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2012, 1642, 1917, 1832, 1979, 1491,  841,  821, 2416, 2397, 1308,\n",
       "       1428, 1843, 1870,  564, 1895, 1080,  823,  353, 1529,  802, 2179,\n",
       "        765,  420, 1284,  696,  440,  182,  950,  203, 1319,  593, 1151,\n",
       "       1977, 1910, 1670,  996, 1616,  527, 1801, 1161, 1288, 1313, 1658,\n",
       "       1216, 1732,  843, 1326, 1923, 3976, 1946, 1691,  188, 1153, 2032,\n",
       "       2026, 1195, 1992, 1094, 1350, 1172, 1464,  212, 1239,  726, 1215,\n",
       "       1206,  431,  688, 1187,  798, 1185, 1154, 1159, 1439, 1729, 2014,\n",
       "        914, 2146, 1421,  382, 2388, 1727, 1972, 2015, 2174, 2067,  950,\n",
       "       1798, 1123,  765, 1887, 1961, 1580, 1127,  418,  361,  440,  262,\n",
       "        285,  698, 1147, 1099, 2015,  675, 1026, 1013, 1764, 1296,  791,\n",
       "       1150])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing no of cleaned words of each article into an array\n",
    "cleaned_words = []\n",
    "for i in cleaned_content:\n",
    "    cleaned_words.append(len(word_tokenize(i)))\n",
    "cleaned_words = np.array(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing positive words\n",
    "with open(r'C:\\Users\\Vishnu\\Downloads/positive-words.txt','r') as f:\n",
    "    s = f.read()\n",
    "positive_words = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing negative words\n",
    "with open(r'C:\\Users\\Vishnu\\Downloads/negative-words.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "negative_words = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive score\n",
    "positive_score = [0] * len(content)\n",
    "for i in range(len(content)):\n",
    "    for word in positive_words:\n",
    "        for letter in cleaned_content[i].lower().split(' '):\n",
    "            if letter == word:\n",
    "                positive_score[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative score\n",
    "negative_score = [0] * len(content)\n",
    "for i in range(len(content)):\n",
    "    for word in negative_words:\n",
    "        for letter in cleaned_content[i].lower().split(' '):\n",
    "            if letter == word:\n",
    "                negative_score[i]+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POSITIVE SCORE'] = positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NEGATIVE SCORE'] = negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUBJECTIVITY SCORE'] = [(df['POSITIVE SCORE'] + df['NEGATIVE SCORE'])/(x + 0.000001) for x in list(cleaned_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG SENTENCE LENGTH'] = words/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining complex words\n",
    "import syllables\n",
    "\n",
    "complex_counts = []\n",
    "syllable_counts = []\n",
    "\n",
    "for sentence in content:\n",
    "  words_ = sentence.split()\n",
    "  # Counting the number of complex words in the sentence\n",
    "  complex_count = sum(syllables.estimate(word) > 2 for word in words_)\n",
    "  complex_counts.append(complex_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_counts = np.array(complex_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERCENTAGE OF COMPLEX WORDS'] = np.array(complex_counts)/np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FOG INDEX'] = 0.4 * (df['AVG SENTENCE LENGTH'] + df['PERCENTAGE OF COMPLEX WORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG NUMBER OF WORDS PER SENTENCES'] = words/sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['COMPLEX WORD COUNT'] = complex_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WORD COUNT'] = cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables_per_word = []\n",
    "\n",
    "for sentence in content:\n",
    "  words = sentence.split()\n",
    "  syllable_counts = [syllables.estimate(word) for word in words]\n",
    "  syllables_per_word.append(sum(syllable_counts) / len(syllable_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SYALLBLE PER WORD'] = syllables_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personal pronouns\n",
    "import re\n",
    "\n",
    "pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "pronoun_counts = []\n",
    "\n",
    "for sentence in content:\n",
    "  pronouns = re.findall(pattern, sentence)\n",
    "  pronoun_count = len(pronouns)\n",
    "  pronoun_counts.append(pronoun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  6,  2, 17, 12, 18,  7,  2,  9,  1,  8,  6, 33, 11,  0, 15,  0,\n",
       "        0,  2,  2,  3,  9,  4,  0,  4,  1,  0,  1, 13,  1,  0,  8,  5, 15,\n",
       "       17, 16,  0,  4,  8,  7, 11,  5,  0,  6, 13,  5,  1,  1,  5,  0, 10,\n",
       "        0,  2,  1,  5,  5,  3,  1,  4, 27, 12,  2,  2, 11,  4,  2,  5,  0,\n",
       "        3,  3, 37,  0,  0,  1,  7,  2,  2, 42,  7,  7,  1,  6, 34, 12,  7,\n",
       "        7,  2,  3,  6,  3,  1,  2,  0,  6,  0,  4,  7,  4,  3,  6,  2,  0,\n",
       "        4,  0,  1,  2,  9,  1,  2,  0,  8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_counts = np.array(pronoun_counts)\n",
    "pronoun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PERSONAL PRONOUNS'] = pronoun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_lengths = []\n",
    "\n",
    "for sentence in content:\n",
    "  words = sentence.split()\n",
    "  char = sum(len(word) for word in words) # stores total characters\n",
    "  avg_word_length = char / len(words)\n",
    "  avg_word_lengths.append(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AVG WORD LENGTH'] = avg_word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCES</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYALLBLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>0      0.057283\n",
       "1      0.056192\n",
       "2      0.06001...</td>\n",
       "      <td>26.129870</td>\n",
       "      <td>0.259940</td>\n",
       "      <td>10.555924</td>\n",
       "      <td>26.129870</td>\n",
       "      <td>523</td>\n",
       "      <td>1833</td>\n",
       "      <td>1.974302</td>\n",
       "      <td>1</td>\n",
       "      <td>5.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>67</td>\n",
       "      <td>36</td>\n",
       "      <td>0      0.072165\n",
       "1      0.070790\n",
       "2      0.07560...</td>\n",
       "      <td>20.525000</td>\n",
       "      <td>0.165043</td>\n",
       "      <td>8.276017</td>\n",
       "      <td>20.525000</td>\n",
       "      <td>271</td>\n",
       "      <td>1455</td>\n",
       "      <td>1.678242</td>\n",
       "      <td>6</td>\n",
       "      <td>5.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>73</td>\n",
       "      <td>37</td>\n",
       "      <td>0      0.060484\n",
       "1      0.059332\n",
       "2      0.06336...</td>\n",
       "      <td>22.552941</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>9.120707</td>\n",
       "      <td>22.552941</td>\n",
       "      <td>477</td>\n",
       "      <td>1736</td>\n",
       "      <td>1.940588</td>\n",
       "      <td>2</td>\n",
       "      <td>5.539412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>79</td>\n",
       "      <td>21</td>\n",
       "      <td>0      0.062950\n",
       "1      0.061751\n",
       "2      0.06594...</td>\n",
       "      <td>19.284211</td>\n",
       "      <td>0.169214</td>\n",
       "      <td>7.781370</td>\n",
       "      <td>19.284211</td>\n",
       "      <td>310</td>\n",
       "      <td>1668</td>\n",
       "      <td>1.697576</td>\n",
       "      <td>17</td>\n",
       "      <td>4.916970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>0      0.057915\n",
       "1      0.056812\n",
       "2      0.06067...</td>\n",
       "      <td>25.050633</td>\n",
       "      <td>0.189490</td>\n",
       "      <td>10.096049</td>\n",
       "      <td>25.050633</td>\n",
       "      <td>375</td>\n",
       "      <td>1813</td>\n",
       "      <td>1.771708</td>\n",
       "      <td>12</td>\n",
       "      <td>5.249569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>0      0.112540\n",
       "1      0.110397\n",
       "2      0.11789...</td>\n",
       "      <td>20.673469</td>\n",
       "      <td>0.216190</td>\n",
       "      <td>8.355864</td>\n",
       "      <td>20.673469</td>\n",
       "      <td>219</td>\n",
       "      <td>933</td>\n",
       "      <td>1.829135</td>\n",
       "      <td>9</td>\n",
       "      <td>5.492881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>0      0.063946\n",
       "1      0.062728\n",
       "2      0.06699...</td>\n",
       "      <td>28.451613</td>\n",
       "      <td>0.193311</td>\n",
       "      <td>11.457969</td>\n",
       "      <td>28.451613</td>\n",
       "      <td>341</td>\n",
       "      <td>1642</td>\n",
       "      <td>1.780284</td>\n",
       "      <td>1</td>\n",
       "      <td>5.304768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>0      0.088161\n",
       "1      0.086482\n",
       "2      0.09235...</td>\n",
       "      <td>19.636364</td>\n",
       "      <td>0.232253</td>\n",
       "      <td>7.947447</td>\n",
       "      <td>19.636364</td>\n",
       "      <td>301</td>\n",
       "      <td>1191</td>\n",
       "      <td>1.826125</td>\n",
       "      <td>2</td>\n",
       "      <td>5.222318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0      0.142663\n",
       "1      0.139946\n",
       "2      0.14945...</td>\n",
       "      <td>27.275862</td>\n",
       "      <td>0.286979</td>\n",
       "      <td>11.025136</td>\n",
       "      <td>27.275862</td>\n",
       "      <td>227</td>\n",
       "      <td>736</td>\n",
       "      <td>1.998603</td>\n",
       "      <td>0</td>\n",
       "      <td>5.811453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "      <td>0      0.100287\n",
       "1      0.098376\n",
       "2      0.10506...</td>\n",
       "      <td>17.424242</td>\n",
       "      <td>0.218261</td>\n",
       "      <td>7.057001</td>\n",
       "      <td>17.424242</td>\n",
       "      <td>251</td>\n",
       "      <td>1047</td>\n",
       "      <td>1.820268</td>\n",
       "      <td>8</td>\n",
       "      <td>5.137667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  \\\n",
       "0                74              31   \n",
       "1                67              36   \n",
       "2                73              37   \n",
       "3                79              21   \n",
       "4                65              25   \n",
       "..              ...             ...   \n",
       "109              25              27   \n",
       "110              44              13   \n",
       "111              30              45   \n",
       "112              34               4   \n",
       "113              43              40   \n",
       "\n",
       "                                    SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0    0      0.057283\n",
       "1      0.056192\n",
       "2      0.06001...            26.129870   \n",
       "1    0      0.072165\n",
       "1      0.070790\n",
       "2      0.07560...            20.525000   \n",
       "2    0      0.060484\n",
       "1      0.059332\n",
       "2      0.06336...            22.552941   \n",
       "3    0      0.062950\n",
       "1      0.061751\n",
       "2      0.06594...            19.284211   \n",
       "4    0      0.057915\n",
       "1      0.056812\n",
       "2      0.06067...            25.050633   \n",
       "..                                                 ...                  ...   \n",
       "109  0      0.112540\n",
       "1      0.110397\n",
       "2      0.11789...            20.673469   \n",
       "110  0      0.063946\n",
       "1      0.062728\n",
       "2      0.06699...            28.451613   \n",
       "111  0      0.088161\n",
       "1      0.086482\n",
       "2      0.09235...            19.636364   \n",
       "112  0      0.142663\n",
       "1      0.139946\n",
       "2      0.14945...            27.275862   \n",
       "113  0      0.100287\n",
       "1      0.098376\n",
       "2      0.10506...            17.424242   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                       0.259940  10.555924   \n",
       "1                       0.165043   8.276017   \n",
       "2                       0.248826   9.120707   \n",
       "3                       0.169214   7.781370   \n",
       "4                       0.189490  10.096049   \n",
       "..                           ...        ...   \n",
       "109                     0.216190   8.355864   \n",
       "110                     0.193311  11.457969   \n",
       "111                     0.232253   7.947447   \n",
       "112                     0.286979  11.025136   \n",
       "113                     0.218261   7.057001   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCES  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                            26.129870                 523        1833   \n",
       "1                            20.525000                 271        1455   \n",
       "2                            22.552941                 477        1736   \n",
       "3                            19.284211                 310        1668   \n",
       "4                            25.050633                 375        1813   \n",
       "..                                 ...                 ...         ...   \n",
       "109                          20.673469                 219         933   \n",
       "110                          28.451613                 341        1642   \n",
       "111                          19.636364                 301        1191   \n",
       "112                          27.275862                 227         736   \n",
       "113                          17.424242                 251        1047   \n",
       "\n",
       "     SYALLBLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             1.974302                  1         5.800000  \n",
       "1             1.678242                  6         5.007087  \n",
       "2             1.940588                  2         5.539412  \n",
       "3             1.697576                 17         4.916970  \n",
       "4             1.771708                 12         5.249569  \n",
       "..                 ...                ...              ...  \n",
       "109           1.829135                  9         5.492881  \n",
       "110           1.780284                  1         5.304768  \n",
       "111           1.826125                  2         5.222318  \n",
       "112           1.998603                  0         5.811453  \n",
       "113           1.820268                  8         5.137667  \n",
       "\n",
       "[111 rows x 14 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:\\Me\\Python stuff/outPut.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266047e1e19e7a5d07d4b44d3ba3a02f46df8073862ebd66a39395c827197c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
